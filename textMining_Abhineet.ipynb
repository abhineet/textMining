{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from collections import Counter\n",
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    f = open(path, \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    data = []\n",
    "    for l in lines:\n",
    "        labelSplit = l.replace('\\n','').split(' ', 1)\n",
    "        data.append([labelSplit[0], [word.lower() for word in labelSplit[1].split()]])\n",
    "    return data\n",
    "\n",
    "data = read_data('./questions.txt')\n",
    "data_train = read_data('./training_data.txt')\n",
    "data_test = read_data('./testing_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data, path):\n",
    "    stop_words = []\n",
    "    with open(path) as f:\n",
    "        stop_words = [word for line in f for word in line.split(\",\")]\n",
    "    data_without_stop_words = []\n",
    "    for k, v in data:\n",
    "        words = [t for t in v if t not in stop_words]\n",
    "        data_without_stop_words.append((k, words))\n",
    "    return data_without_stop_words\n",
    "\n",
    "\n",
    "data = remove_stop_words(data, './stop_words.txt')\n",
    "data_train = remove_stop_words(data_train, './stop_words.txt')\n",
    "data_test = remove_stop_words(data_test, './stop_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(data):\n",
    "    _labels = []\n",
    "    for k,v in data:\n",
    "        _labels.append(k)   \n",
    "    _unique_label = list(set(_labels))\n",
    "    _unique_label_dict = {}\n",
    "    for k,v in enumerate(_unique_label):\n",
    "        _unique_label_dict[v] = k\n",
    "    return _unique_label_dict\n",
    "\n",
    "labels = get_labels(data)\n",
    "labels_train = get_labels(data_train)\n",
    "labels_test = get_labels(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_labels(data, labels):        \n",
    "    cleaned_data = []\n",
    "    for k,v in data:\n",
    "        cleaned_data.append((labels[k],v))\n",
    "        \n",
    "    return np.array(cleaned_data)\n",
    "\n",
    "data = append_labels(data, labels)\n",
    "data_train = append_labels(data_train, labels_train)\n",
    "data_test = append_labels(data_test,labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[35, list(['how', 'serfdom', 'develop', 'leave', 'russia', '?'])],\n",
       "       [0,\n",
       "        list(['what', 'films', 'featured', 'character', 'popeye', 'doyle', '?'])],\n",
       "       [35,\n",
       "        list(['how', 'find', 'list', 'celebrities', \"'\", 'real', 'names', '?'])],\n",
       "       [46,\n",
       "        list(['what', 'fowl', 'grabs', 'spotlight', 'chinese', 'year', 'monkey', '?'])],\n",
       "       [12, list(['what', 'full', 'form', '.com', '?'])]], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[15, list(['what', 'chiricahua', 'name', '?'])],\n",
       "       [27,\n",
       "        list(['what', \"'s\", 'last', 'line', 'dickens', \"'s\", 'christmas', 'carol', '?'])],\n",
       "       [31, list(['what', 'names', 'andrew', 'christina', 'mean', '?'])],\n",
       "       [4,\n",
       "        list(['where', 'song', 'anything', 'goes', 'take', 'place', '?'])],\n",
       "       [29, list(['how', 'clean', 'cache', '?'])]], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44, list(['how', 'many', 'varieties', 'twins', '?'])],\n",
       "       [0,\n",
       "        list(['what', 'tv', 'quiz', 'show', 'left', 'air', '1975', 'tune', 'vera', 'lynn', \"'s\", \"'ll\", 'meet', '?'])],\n",
       "       [26, list(['what', 'oldest', 'profession', '?'])],\n",
       "       [38, list(['what', 'feudal', 'system', '?'])],\n",
       "       [0, list(['jude', 'law', 'what', 'movie', '?'])]], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indexed_vocab(data):\n",
    "    vocab = []\n",
    "    for _, sent in data:\n",
    "        for word in sent:\n",
    "            vocab.append(word)\n",
    "    count = Counter(vocab)\n",
    "    count = {w : count[w] for w in count if count[w] >= 2}\n",
    "    vocab = []\n",
    "    for k, v in count.items():\n",
    "        vocab.append(k)\n",
    "    indexed_vocab = {word: idx for idx, word in enumerate(vocab)}\n",
    "    return indexed_vocab\n",
    "def create_vocab(data):\n",
    "    total_words_orig = []\n",
    "    for k,sent in data:\n",
    "        for word in sent:\n",
    "            total_words_orig.append(word)\n",
    "    total_words = list(set(total_words_orig))\n",
    "    total_words_str = ' '.join(total_words)\n",
    "    vocab = set(total_words_str.split()) \n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)} # create word index\n",
    "    return word2idx\n",
    "word2idx=create_vocab(data)\n",
    "word2idx['#UNK#'] = len(word2idx)\n",
    "indexed_vocab = create_indexed_vocab(data)\n",
    "indexed_vocab['#UNK#'] = len(indexed_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path, indexed_vocab, embedding_dim=300):\n",
    "    with open(path) as f:\n",
    "        embeddings = np.zeros((len(indexed_vocab), embedding_dim))\n",
    "        for line in f.readlines():\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            index = indexed_vocab.get(word)\n",
    "            if index:\n",
    "                vector = np.array(values[1:], dtype='float32')\n",
    "                embeddings[index] = vector\n",
    "        return torch.from_numpy(embeddings).float()\n",
    "\n",
    "glove_random = load_glove_embeddings('./glove.small.txt', indexed_vocab)\n",
    "glove_pre = load_glove_embeddings('./glove.small.txt', word2idx)\n",
    "embeddings_random = nn.Embedding(glove_random.size(0), glove_random.size(1))\n",
    "embeddings_pretrained = nn.Embedding.from_pretrained(glove_pre, freeze=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_train_test(data, test_ratio):\n",
    "#     data_copy = copy.deepcopy(data)\n",
    "#     np.random.shuffle(data_copy)\n",
    "#     test_set_size = int(len(data) * test_ratio)\n",
    "#     test = data_copy[:test_set_size]\n",
    "#     train = data_copy[test_set_size:]\n",
    "#     return train, test\n",
    "\n",
    "# train, test = split_train_test(data, 0.2)\n",
    "# train,dev =split_train_test(train,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44, list(['how', 'many', 'varieties', 'twins', '?'])],\n",
       "       [0,\n",
       "        list(['what', 'tv', 'quiz', 'show', 'left', 'air', '1975', 'tune', 'vera', 'lynn', \"'s\", \"'ll\", 'meet', '?'])],\n",
       "       [26, list(['what', 'oldest', 'profession', '?'])],\n",
       "       [38, list(['what', 'feudal', 'system', '?'])]], dtype=object)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWClassifier(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size, num_labels):\n",
    "        super(BOWClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(self.input_size, num_labels)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(x)\n",
    "        #output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, indexed_vocab,embeddings):\n",
    "    pt_tensor= torch.zeros(300, dtype=torch.long)\n",
    "    count = 0\n",
    "    for word in sentence:\n",
    "        count += 1\n",
    "        if word in indexed_vocab:\n",
    "            pt_tensor = torch.add(pt_tensor, embeddings(torch.LongTensor([indexed_vocab[word]]))[0])\n",
    "        else:\n",
    "            pt_tensor = torch.add(pt_tensor, embeddings(torch.LongTensor([indexed_vocab['#UNK#']]))[0])\n",
    "    pt_tensor=torch.div(pt_tensor, count)\n",
    "    return pt_tensor\n",
    "\n",
    "def get_bow_rep(data,word2idx,embeddings):\n",
    "    bow_data = []\n",
    "    for label, sent in data:\n",
    "        bow_data.append(make_bow_vector(sent, word2idx,embeddings))\n",
    "    return torch.stack(bow_data)\n",
    "        \n",
    "training_set = get_bow_rep(data_train,word2idx,embeddings_pretrained)\n",
    "training_set_rand = get_bow_rep(data_train,indexed_vocab,embeddings_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = 300\n",
    "hidden_size = 100\n",
    "num_classes = 50\n",
    "num_epochs = 350\n",
    "learning_rate = 0.0001\n",
    "batch_size = 100\n",
    "\n",
    "model = BOWClassifier(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) \n",
    "#training in batches\n",
    "for epoch in range(num_epochs):\n",
    "    permutation = torch.randperm(training_set.size()[0])\n",
    "    for i in range(0, training_set.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        batch_features = training_set[indices]\n",
    "        batch_features = batch_features.reshape(-1, 300).to(device)\n",
    "        batch_labels = torch.LongTensor([label for label, sent in data_train[indices]]).to(device)\n",
    "        batch_outputs = model(batch_features)\n",
    "        loss = criterion(batch_outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(loss.item())\n",
    "    print(epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "test_len =len(data_test)\n",
    "correct=0\n",
    "for label,data in data_test:\n",
    "    bow_vec = make_bow_vector(data, word2idx,embeddings_pretrained)\n",
    "    logprobs = model(bow_vec)\n",
    "    logprobs = F.softmax(logprobs)\n",
    "    pred = np.argmax(logprobs.data.numpy())\n",
    "    if pred==label:\n",
    "        correct+=1\n",
    "    print('prediction: {}'.format(pred))\n",
    "    print('actual: {}'.format(label))\n",
    "accuracy = correct/test_len\n",
    "print('accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        # Decode the hidden state of the last time step\n",
    "        tag_space = self.hidden2tag(lstm_out[-1, :])\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM training\n",
    "vocab_size=glove_random.size(0)\n",
    "embedding_dim=glove_random.size(1)\n",
    "tagset_size=51\n",
    "hidden_dim=300\n",
    "learning_rate=0.0001\n",
    "ModelLSTM = LSTMTagger(embedding_dim,hidden_dim,vocab_size,tagset_size)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ModelLSTM.parameters(), lr=learning_rate)\n",
    "num_epochs=2\n",
    "for epoch in range(num_epochs):\n",
    "    for label,sent in data_train:\n",
    "        ids=[]\n",
    "        unk = 3375\n",
    "        for word in sent:\n",
    "            index =indexed_vocab.get(word)\n",
    "            if index:\n",
    "                ids.append([indexed_vocab[word]])\n",
    "            else:\n",
    "                ids.append([unk])\n",
    "        batch_labels=torch.tensor([label])\n",
    "        sentence=torch.tensor(ids).squeeze()\n",
    "        optimizer.zero_grad()\n",
    "        batch_outputs = ModelLSTM(sentence)\n",
    "        loss = criterion(batch_outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(\"loss\",loss.item())\n",
    "    print(\"epoch\",epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM testing\n",
    "test_len =len(data_test)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    for label,sent in data_test:\n",
    "        ids=[]\n",
    "        unk = 3375\n",
    "        labels=[]\n",
    "        for word in sent:\n",
    "            index =indexed_vocab.get(word)\n",
    "            if index:\n",
    "                ids.append([indexed_vocab[word]])\n",
    "            else:\n",
    "                ids.append([unk])\n",
    "        sentence=torch.tensor(ids).squeeze()\n",
    "        batch_outputs = ModelLSTM(sentence)\n",
    "        predicted = np.argmax(F.softmax(batch_outputs).data.numpy())\n",
    "        if predicted==label:\n",
    "             correct+=1\n",
    "        print('prediction: {}'.format(predicted))\n",
    "        print('actual: {}'.format(label))\n",
    "    accuracy = correct/test_len\n",
    "    print('accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4416"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
