{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from collections import Counter\n",
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn as nn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    f = open(path, \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    data = []\n",
    "    for l in lines:\n",
    "        labelSplit = l.replace('\\n','').split(' ', 1)\n",
    "        data.append([labelSplit[0], [word.lower() for word in labelSplit[1].split()]])\n",
    "    return data\n",
    "\n",
    "data = read_data('./questions.txt')\n",
    "data_train = read_data('./training_data.txt')\n",
    "data_test = read_data('./testing_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data, path):\n",
    "    stop_words = []\n",
    "    with open(path) as f:\n",
    "        stop_words = [word for line in f for word in line.split(\",\")]\n",
    "    data_without_stop_words = []\n",
    "    for k, v in data:\n",
    "        words = [t for t in v if t not in stop_words]\n",
    "        data_without_stop_words.append((k, words))\n",
    "    return data_without_stop_words\n",
    "\n",
    "\n",
    "data = remove_stop_words(data, './stop_words.txt')\n",
    "data_train = remove_stop_words(data_train, './stop_words.txt')\n",
    "data_test = remove_stop_words(data_test, './stop_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(data):\n",
    "    _labels = []\n",
    "    for k,v in data:\n",
    "        _labels.append(k)   \n",
    "    _unique_label = list(set(_labels))\n",
    "    _unique_label_dict = {}\n",
    "    for k,v in enumerate(_unique_label):\n",
    "        _unique_label_dict[v] = k\n",
    "    return _unique_label_dict\n",
    "\n",
    "labels = get_labels(data)\n",
    "labels_train = get_labels(data_train)\n",
    "labels_test = get_labels(data_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "51"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ENTY:plant': 0,\n",
       " 'DESC:desc': 1,\n",
       " 'HUM:desc': 2,\n",
       " 'HUM:ind': 3,\n",
       " 'NUM:dist': 4,\n",
       " 'NUM:money': 5,\n",
       " 'NUM:weight': 6,\n",
       " 'ENTY:body': 7,\n",
       " 'ENTY:sport': 8,\n",
       " 'ENTY:instru': 9,\n",
       " 'HUM:title': 10,\n",
       " 'NUM:speed': 11,\n",
       " 'LOC:country': 12,\n",
       " 'DESC:manner': 13,\n",
       " 'NUM:code': 14,\n",
       " 'ENTY:letter': 15,\n",
       " 'ENTY:food': 16,\n",
       " 'LOC:mount': 17,\n",
       " 'ENTY:substance': 18,\n",
       " 'ENTY:veh': 19,\n",
       " 'ENTY:currency': 20,\n",
       " 'ENTY:word': 21,\n",
       " 'LOC:city': 22,\n",
       " 'DESC:def': 23,\n",
       " 'DESC:reason': 24,\n",
       " 'ENTY:lang': 25,\n",
       " 'NUM:perc': 26,\n",
       " 'ENTY:event': 27,\n",
       " 'ABBR:exp': 28,\n",
       " 'ABBR:abb': 29,\n",
       " 'NUM:volsize': 30,\n",
       " 'ENTY:symbol': 31,\n",
       " 'ENTY:termeq': 32,\n",
       " 'NUM:period': 33,\n",
       " 'NUM:date': 34,\n",
       " 'LOC:state': 35,\n",
       " 'ENTY:cremat': 36,\n",
       " 'LOC:other': 37,\n",
       " 'ENTY:product': 38,\n",
       " 'HUM:gr': 39,\n",
       " 'NUM:other': 40,\n",
       " 'ENTY:other': 41,\n",
       " '\\ufeffDESC:manner': 42,\n",
       " 'ENTY:religion': 43,\n",
       " 'NUM:temp': 44,\n",
       " 'NUM:count': 45,\n",
       " 'NUM:ord': 46,\n",
       " 'ENTY:dismed': 47,\n",
       " 'ENTY:animal': 48,\n",
       " 'ENTY:techmeth': 49,\n",
       " 'ENTY:color': 50}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_labels(data, labels):        \n",
    "    cleaned_data = []\n",
    "    for k,v in data:\n",
    "        cleaned_data.append((labels[k],v))\n",
    "        \n",
    "    return np.array(cleaned_data)\n",
    "\n",
    "data = append_labels(data, labels)\n",
    "data_train = append_labels(data_train, labels)\n",
    "data_test = append_labels(data_test,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[42, list(['how', 'serfdom', 'develop', 'leave', 'russia', '?'])],\n",
       "       [36,\n",
       "        list(['what', 'films', 'featured', 'character', 'popeye', 'doyle', '?'])],\n",
       "       [13,\n",
       "        list(['how', 'find', 'list', 'celebrities', \"'\", 'real', 'names', '?'])],\n",
       "       [48,\n",
       "        list(['what', 'fowl', 'grabs', 'spotlight', 'chinese', 'year', 'monkey', '?'])],\n",
       "       [28, list(['what', 'full', 'form', '.com', '?'])]], dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[32, list(['what', 'chiricahua', 'name', '?'])],\n",
       "       [1,\n",
       "        list(['what', \"'s\", 'last', 'line', 'dickens', \"'s\", 'christmas', 'carol', '?'])],\n",
       "       [23, list(['what', 'names', 'andrew', 'christina', 'mean', '?'])],\n",
       "       [37,\n",
       "        list(['where', 'song', 'anything', 'goes', 'take', 'place', '?'])],\n",
       "       [13, list(['how', 'clean', 'cache', '?'])]], dtype=object)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45, list(['how', 'many', 'varieties', 'twins', '?'])],\n",
       "       [36,\n",
       "        list(['what', 'tv', 'quiz', 'show', 'left', 'air', '1975', 'tune', 'vera', 'lynn', \"'s\", \"'ll\", 'meet', '?'])],\n",
       "       [10, list(['what', 'oldest', 'profession', '?'])],\n",
       "       [23, list(['what', 'feudal', 'system', '?'])],\n",
       "       [36, list(['jude', 'law', 'what', 'movie', '?'])]], dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indexed_vocab(data):\n",
    "    vocab = []\n",
    "    for _, sent in data:\n",
    "        for word in sent:\n",
    "            vocab.append(word)\n",
    "    count = Counter(vocab)\n",
    "    count = {w : count[w] for w in count if count[w] >= 2}\n",
    "    vocab = []\n",
    "    for k, v in count.items():\n",
    "        vocab.append(k)\n",
    "    indexed_vocab = {word: idx for idx, word in enumerate(vocab)}\n",
    "    return indexed_vocab\n",
    "def create_vocab(data):\n",
    "    total_words_orig = []\n",
    "    for k,sent in data:\n",
    "        for word in sent:\n",
    "            total_words_orig.append(word)\n",
    "    total_words = list(set(total_words_orig))\n",
    "    total_words_str = ' '.join(total_words)\n",
    "    vocab = set(total_words_str.split()) \n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)} # create word index\n",
    "    return word2idx\n",
    "word2idx=create_vocab(data)\n",
    "word2idx['#UNK#'] = len(word2idx)\n",
    "indexed_vocab = create_indexed_vocab(data)\n",
    "indexed_vocab['#UNK#'] = len(indexed_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path, indexed_vocab, embedding_dim=300):\n",
    "    with open(path) as f:\n",
    "        embeddings = np.zeros((len(indexed_vocab), embedding_dim))\n",
    "        for line in f.readlines():\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            index = indexed_vocab.get(word)\n",
    "            if index:\n",
    "                vector = np.array(values[1:], dtype='float32')\n",
    "                embeddings[index] = vector\n",
    "        return torch.from_numpy(embeddings).float()\n",
    "\n",
    "glove_random = load_glove_embeddings('./glove.small.txt', indexed_vocab)\n",
    "glove_pre = load_glove_embeddings('./glove.small.txt', word2idx)\n",
    "embeddings_random = nn.Embedding(glove_random.size(0), glove_random.size(1))\n",
    "embeddings_pretrained = nn.Embedding.from_pretrained(glove_pre, freeze=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def split_train_test(data, test_ratio):\n",
    "#     data_copy = copy.deepcopy(data)\n",
    "#     np.random.shuffle(data_copy)\n",
    "#     test_set_size = int(len(data) * test_ratio)\n",
    "#     test = data_copy[:test_set_size]\n",
    "#     train = data_copy[test_set_size:]\n",
    "#     return train, test\n",
    "\n",
    "# train, test = split_train_test(data, 0.2)\n",
    "# train,dev =split_train_test(train,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[45, list(['how', 'many', 'varieties', 'twins', '?'])],\n",
       "       [36,\n",
       "        list(['what', 'tv', 'quiz', 'show', 'left', 'air', '1975', 'tune', 'vera', 'lynn', \"'s\", \"'ll\", 'meet', '?'])],\n",
       "       [10, list(['what', 'oldest', 'profession', '?'])],\n",
       "       [23, list(['what', 'feudal', 'system', '?'])]], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWClassifier(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size, num_labels):\n",
    "        super(BOWClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(self.input_size, num_labels)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(x)\n",
    "        #output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, indexed_vocab,embeddings):\n",
    "    pt_tensor= torch.zeros(300, dtype=torch.long)\n",
    "    count = 0\n",
    "    for word in sentence:\n",
    "        count += 1\n",
    "        if word in indexed_vocab:\n",
    "            pt_tensor = torch.add(pt_tensor, embeddings(torch.LongTensor([indexed_vocab[word]]))[0])\n",
    "        else:\n",
    "            pt_tensor = torch.add(pt_tensor, embeddings(torch.LongTensor([indexed_vocab['#UNK#']]))[0])\n",
    "    pt_tensor=torch.div(pt_tensor, count)\n",
    "    return pt_tensor\n",
    "\n",
    "def get_bow_rep(data,word2idx,embeddings):\n",
    "    bow_data = []\n",
    "    for label, sent in data:\n",
    "        bow_data.append(make_bow_vector(sent, word2idx,embeddings))\n",
    "    return torch.stack(bow_data)\n",
    "        \n",
    "training_set = get_bow_rep(data_train,word2idx,embeddings_pretrained)\n",
    "training_set_rand = get_bow_rep(data_train,indexed_vocab,embeddings_random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranitsinha/opt/anaconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:8: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy= 65.3211009174312\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 66.97247706422019\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 69.72477064220183\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 71.92660550458716\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 71.37614678899082\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 60.91743119266055\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 56.513761467889914\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 48.62385321100918\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 55.779816513761475\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 37.06422018348624\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 36.69724770642202\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 26.238532110091743\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 26.055045871559635\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 9.174311926605505\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 2.5688073394495414\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 5.5045871559633035\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 63.6697247706422\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 66.97247706422019\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 65.87155963302752\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 66.60550458715596\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 68.25688073394495\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 72.47706422018348\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 73.21100917431193\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 73.39449541284404\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 73.02752293577981\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 70.09174311926606\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 67.1559633027523\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 63.30275229357798\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 66.05504587155964\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 44.587155963302756\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 37.61467889908257\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 40.18348623853211\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 63.30275229357798\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 65.87155963302752\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 66.23853211009174\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 67.52293577981652\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 67.88990825688074\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 71.55963302752293\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 71.74311926605505\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 72.8440366972477\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 73.39449541284404\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 71.37614678899082\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 70.64220183486239\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 67.3394495412844\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 69.35779816513762\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 53.94495412844037\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 46.23853211009175\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 41.10091743119266\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 63.853211009174316\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 67.3394495412844\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 65.68807339449542\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 66.23853211009174\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 66.78899082568807\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 68.9908256880734\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 70.45871559633028\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 71.92660550458716\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 71.55963302752293\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 73.57798165137615\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 73.21100917431193\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 71.19266055045873\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 71.74311926605505\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 62.75229357798165\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 56.88073394495413\n",
      "\n",
      "\n",
      "hidden_size=50\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 48.62385321100918\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 65.13761467889908\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 68.62385321100918\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 70.27522935779817\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 70.27522935779817\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 72.11009174311927\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 61.8348623853211\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 55.596330275229356\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 49.54128440366973\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 55.596330275229356\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 38.899082568807344\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 38.71559633027523\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 33.39449541284404\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 23.30275229357798\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 6.605504587155964\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 7.706422018348624\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 7.339449541284404\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 65.3211009174312\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 66.97247706422019\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 66.97247706422019\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 67.88990825688074\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 68.80733944954129\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 72.47706422018348\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 73.39449541284404\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy= 73.39449541284404\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 73.21100917431193\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 70.27522935779817\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 67.52293577981652\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 63.11926605504588\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 66.23853211009174\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 45.50458715596331\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 39.816513761467895\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 36.3302752293578\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 65.13761467889908\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 66.05504587155964\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 64.58715596330276\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 65.87155963302752\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 68.25688073394495\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 71.55963302752293\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 72.11009174311927\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 73.57798165137615\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 73.57798165137615\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 71.55963302752293\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 70.45871559633028\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 67.1559633027523\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 69.35779816513762\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 53.94495412844037\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 46.055045871559635\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 39.816513761467895\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 63.48623853211009\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 65.68807339449542\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 65.5045871559633\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 66.60550458715596\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 66.78899082568807\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 68.9908256880734\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 70.64220183486239\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 71.92660550458716\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 71.55963302752293\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 73.39449541284404\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 73.21100917431193\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 71.19266055045873\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 71.55963302752293\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 62.38532110091744\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 56.69724770642202\n",
      "\n",
      "\n",
      "hidden_size=100\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 47.88990825688074\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 68.07339449541284\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 67.1559633027523\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 68.80733944954129\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 69.72477064220183\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 72.11009174311927\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 61.8348623853211\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 56.69724770642202\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 48.80733944954129\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 55.04587155963303\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 35.59633027522936\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 34.31192660550459\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 24.954128440366972\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 23.669724770642205\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 8.807339449541285\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 9.908256880733946\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 5.321100917431193\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 61.46788990825688\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 66.60550458715596\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 67.52293577981652\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 66.05504587155964\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 68.9908256880734\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 72.6605504587156\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 73.57798165137615\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 73.21100917431193\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 73.39449541284404\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 70.45871559633028\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 67.3394495412844\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 63.48623853211009\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 66.05504587155964\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 45.50458715596331\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 39.816513761467895\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 38.1651376146789\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 64.22018348623854\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 65.68807339449542\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 66.42201834862385\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 66.23853211009174\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 68.44036697247707\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 71.37614678899082\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 71.92660550458716\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 73.57798165137615\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 73.39449541284404\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 71.37614678899082\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 70.27522935779817\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 67.3394495412844\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 69.1743119266055\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy= 53.57798165137615\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 45.68807339449542\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 40.0\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 63.48623853211009\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 66.05504587155964\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 64.77064220183486\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 66.78899082568807\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 66.42201834862385\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 69.1743119266055\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 70.64220183486239\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 72.11009174311927\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 71.55963302752293\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 73.39449541284404\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 73.02752293577981\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 71.19266055045873\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 71.55963302752293\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 63.11926605504588\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 56.88073394495413\n",
      "\n",
      "\n",
      "hidden_size=200\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 47.706422018348626\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 66.97247706422019\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 70.09174311926606\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 68.80733944954129\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 72.29357798165138\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 71.74311926605505\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 61.100917431192656\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 56.88073394495413\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=0.001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 48.440366972477065\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 55.779816513761475\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 38.1651376146789\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 36.3302752293578\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 17.431192660550458\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 27.522935779816514\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 15.963302752293579\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 5.871559633027523\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=10\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 5.137614678899083\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 64.40366972477064\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 66.42201834862385\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 66.97247706422019\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 66.78899082568807\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 68.80733944954129\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 72.11009174311927\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 73.21100917431193\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=0.001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 73.39449541284404\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 73.21100917431193\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 69.72477064220183\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 66.78899082568807\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 64.03669724770641\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 66.23853211009174\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 44.403669724770644\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 39.816513761467895\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=300\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 37.247706422018354\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 65.87155963302752\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 65.87155963302752\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 66.97247706422019\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 66.60550458715596\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 67.70642201834862\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 71.37614678899082\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 72.11009174311927\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=0.001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 73.39449541284404\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 73.21100917431193\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 71.55963302752293\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 70.64220183486239\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 67.70642201834862\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 69.1743119266055\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 54.12844036697248\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 46.055045871559635\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=500\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 40.0\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 63.11926605504588\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 66.78899082568807\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 66.60550458715596\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=0.1\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 65.68807339449542\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 66.97247706422019\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 69.35779816513762\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 70.45871559633028\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=0.001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 71.92660550458716\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 71.55963302752293\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 73.76146788990826\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 73.02752293577981\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=0.0001\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 71.37614678899082\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=2\n",
      "\n",
      "Accuracy= 71.92660550458716\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=50\n",
      "\n",
      "Accuracy= 62.38532110091744\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=100\n",
      "\n",
      "Accuracy= 56.513761467889914\n",
      "\n",
      "\n",
      "hidden_size=250\n",
      "num_epochs=1000\n",
      "learning_rate=1e-05\n",
      "batch_size=200\n",
      "\n",
      "Accuracy= 48.80733944954129\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "# input_size = 300\n",
    "# hidden_size = 100\n",
    "# num_classes = 51\n",
    "# num_epochs = 350\n",
    "# learning_rate = 0.0001\n",
    "# batch_size = 100\n",
    "accu=[]\n",
    "input_size = 300\n",
    "hidden_size = [50,100,200,250]\n",
    "num_classes = 51\n",
    "num_epochs = [10,300,500,1000]\n",
    "learning_rate = [0.1,0.001,0.0001,0.00001]\n",
    "batch_size = [2,50,100,200]\n",
    "for k in hidden_size:\n",
    "    for j in num_epochs:\n",
    "        for r in learning_rate:\n",
    "            for x in batch_size:\n",
    "                print(\"hidden_size={0}\\nnum_epochs={1}\\nlearning_rate={2}\\nbatch_size={3}\".format(k,j,r,x))\n",
    "                model = BOWClassifier(input_size, k, num_classes).to(device)\n",
    "                criterion = torch.nn.CrossEntropyLoss()\n",
    "                optimizer = torch.optim.Adam(model.parameters(), lr=r) \n",
    "                #training in batches\n",
    "                for epoch in range(j):\n",
    "                    permutation = torch.randperm(training_set.size()[0])\n",
    "                    for i in range(0, training_set.size()[0], x):\n",
    "                        optimizer.zero_grad()\n",
    "                        indices = permutation[i:i + x]\n",
    "                        batch_features = training_set[indices]\n",
    "                        batch_features = batch_features.reshape(-1, 300).to(device)\n",
    "                        batch_labels = torch.LongTensor([label for label, sent in data_train[indices]]).to(device)\n",
    "                        batch_outputs = model(batch_features)\n",
    "                        loss = criterion(batch_outputs, batch_labels)\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "                  #print(loss.item())\n",
    "                    #print(epoch)\n",
    "                #acc.append(value_of_patameter)\n",
    "                accur=accuracy(data_test)*100\n",
    "                accu.append([k,j,r,x,accur])\n",
    "                print(\"\\nAccuracy=\",accur)\n",
    "                print(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[50, 10, 0.1, 2, 65.3211009174312],\n",
       " [50, 10, 0.1, 50, 66.97247706422019],\n",
       " [50, 10, 0.1, 100, 69.72477064220183]]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accu[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.76146788990826\n"
     ]
    }
   ],
   "source": [
    "accuracy_list=[]\n",
    "l_rate=[]\n",
    "for i in range(len(accu)):\n",
    "    accuracy_list.append(accu[i][4])\n",
    "    l_rate.append(accu[i][2])\n",
    "print(max(accuracy_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# plt.scatter(l_rate,accuracy_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size=250\n",
      "Num_epochs=1000\n",
      "learning rate=0.0001\n",
      "Batch Size=50\n",
      "Accuracy=73.76146788990826\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(accu)):\n",
    "    if accu[i][4]==73.76146788990826:\n",
    "        print(\"Hidden size={}\\nNum_epochs={}\\nlearning rate={}\\nBatch Size={}\\nAccuracy={}\".format(accu[i][0],accu[i][1],accu[i][2],accu[i][3],accu[i][4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "def accuracy(data_test):\n",
    "    test_len =len(data_test)\n",
    "    correct=0\n",
    "    for label,data in data_test:\n",
    "        bow_vec = make_bow_vector(data, word2idx,embeddings_pretrained)\n",
    "        logprobs = model(bow_vec)\n",
    "        logprobs = F.softmax(logprobs)\n",
    "        pred = np.argmax(logprobs.data.numpy())\n",
    "        if pred==label:\n",
    "            correct+=1\n",
    "          #print('prediction: {}'.format(pred))\n",
    "        #print('actual: {}'.format(label))\n",
    "    acc = correct/test_len\n",
    "    return acc\n",
    "    #print('accuracy: {}'.format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTagger(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        #self.word_embeddings=nn.Embedding.from_pretrained(glove_pre, freeze=True)\n",
    "\n",
    "\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim*2, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        # Decode the hidden state of the last time step\n",
    "        tag_space = self.hidden2tag(lstm_out[-1, :])\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_epochs=1\n",
      "learning_rate=0.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranitsinha/opt/anaconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:18: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy= 12.477064220183486\n",
      "\n",
      "\n",
      "num_epochs=1\n",
      "learning_rate=0.01\n",
      "\n",
      "Accuracy= 33.211009174311926\n",
      "\n",
      "\n",
      "num_epochs=1\n",
      "learning_rate=0.0001\n",
      "\n",
      "Accuracy= 57.064220183486235\n",
      "\n",
      "\n",
      "num_epochs=2\n",
      "learning_rate=0.1\n",
      "\n",
      "Accuracy= 11.926605504587156\n",
      "\n",
      "\n",
      "num_epochs=2\n",
      "learning_rate=0.01\n",
      "\n",
      "Accuracy= 31.926605504587158\n",
      "\n",
      "\n",
      "num_epochs=2\n",
      "learning_rate=0.0001\n",
      "\n",
      "Accuracy= 63.6697247706422\n",
      "\n",
      "\n",
      "num_epochs=3\n",
      "learning_rate=0.1\n",
      "\n",
      "Accuracy= 13.394495412844037\n",
      "\n",
      "\n",
      "num_epochs=3\n",
      "learning_rate=0.01\n",
      "\n",
      "Accuracy= 36.14678899082569\n",
      "\n",
      "\n",
      "num_epochs=3\n",
      "learning_rate=0.0001\n",
      "\n",
      "Accuracy= 67.1559633027523\n",
      "\n",
      "\n",
      "num_epochs=5\n",
      "learning_rate=0.1\n",
      "\n",
      "Accuracy= 13.761467889908257\n",
      "\n",
      "\n",
      "num_epochs=5\n",
      "learning_rate=0.01\n",
      "\n",
      "Accuracy= 33.94495412844037\n",
      "\n",
      "\n",
      "num_epochs=5\n",
      "learning_rate=0.0001\n",
      "\n",
      "Accuracy= 71.19266055045873\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#LSTM training\n",
    "vocab_size=glove_random.size(0)\n",
    "embedding_dim=glove_random.size(1)\n",
    "tagset_size=51\n",
    "hidden_dim=300\n",
    "learning_rate=[0.1,0.01,0.0001]\n",
    "num_epochs=[1,2,3,5]\n",
    "accur=0\n",
    "accuu=[]\n",
    "for n in num_epochs :\n",
    "    for r in learning_rate:\n",
    "        print(\"num_epochs={0}\\nlearning_rate={1}\".format(n,r))\n",
    "        ModelLSTM = LSTMTagger(embedding_dim,hidden_dim,vocab_size,tagset_size)\n",
    "        criterion = torch.nn.CrossEntropyLoss()\n",
    "        optimizer = torch.optim.Adam(ModelLSTM.parameters(), lr=r)\n",
    "\n",
    "        for epoch in range(n):\n",
    "            for label,sent in data_train:\n",
    "                ids=[]\n",
    "                unk = 3375\n",
    "                for word in sent:\n",
    "                    index =indexed_vocab.get(word)\n",
    "                    if index:\n",
    "                        ids.append([indexed_vocab[word]])\n",
    "                    else:\n",
    "                        ids.append([unk])\n",
    "                batch_labels=torch.tensor([label])\n",
    "                sentence=torch.tensor(ids).squeeze()\n",
    "                optimizer.zero_grad()\n",
    "                batch_outputs = ModelLSTM(sentence)\n",
    "                loss = criterion(batch_outputs, batch_labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "#                 \n",
    "        accur=accuracy(data_test)*100\n",
    "        accuu.append([,accur])\n",
    "        print(\"\\nAccuracy=\",accur)\n",
    "        print(\"\\n\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 0.1, 12.477064220183486],\n",
       " [1, 0.01, 33.211009174311926],\n",
       " [1, 0.0001, 57.064220183486235],\n",
       " [2, 0.1, 11.926605504587156],\n",
       " [2, 0.01, 31.926605504587158],\n",
       " [2, 0.0001, 63.6697247706422],\n",
       " [3, 0.1, 13.394495412844037],\n",
       " [3, 0.01, 36.14678899082569],\n",
       " [3, 0.0001, 67.1559633027523],\n",
       " [5, 0.1, 13.761467889908257],\n",
       " [5, 0.01, 33.94495412844037],\n",
       " [5, 0.0001, 71.19266055045873]]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM testing\n",
    "def lstm_acc(data_test):\n",
    "    test_len =len(data_test)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for label,sent in data_test:\n",
    "            ids=[]\n",
    "            unk = 3375\n",
    "            labels=[]\n",
    "            for word in sent:\n",
    "                index =indexed_vocab.get(word)\n",
    "                if index:\n",
    "                    ids.append([indexed_vocab[word]])\n",
    "                else:\n",
    "                    ids.append([unk])\n",
    "            sentence=torch.tensor(ids).squeeze()\n",
    "            batch_outputs = ModelLSTM(sentence)\n",
    "            predicted = np.argmax(F.softmax(batch_outputs).data.numpy())\n",
    "            if predicted==label:\n",
    "                 correct+=1\n",
    "#             print('prediction: {}'.format(predicted))\n",
    "#             print('actual: {}'.format(label))\n",
    "        accuracy = correct/test_len\n",
    "#         print('accuracy: {}'.format(accuracy))\n",
    "        return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4416"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMTaggerpre(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, glove):\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        super(LSTMTaggerpre, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "      \n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(glove, freeze=True)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim * 2, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        # Decode the hidden state of the last time step\n",
    "        tag_space = self.hidden2tag(lstm_out[-1, :])\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training started !!! It may take a few minutes.\n",
      "training finished\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/pranitsinha/opt/anaconda3/envs/myenv/lib/python3.7/site-packages/ipykernel_launcher.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy= 86.05504587155963\n"
     ]
    }
   ],
   "source": [
    "vocab_size=glove_pre.size(0)\n",
    "embedding_dim=glove_pre.size(1)\n",
    "tagset_size=51\n",
    "hidden_dim=300\n",
    "learning_rate=0.0001\n",
    "num_epochs=5\n",
    "print(\"Training started !!! It may take a few minutes.\")\n",
    "\n",
    "ModelLSTMM = LSTMTaggerpre(embedding_dim, hidden_dim, vocab_size, tagset_size, glove_pre)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(ModelLSTMM.parameters(), lr=learning_rate)\n",
    "\n",
    "unk = word2idx.get('#UNK#')\n",
    "for epoch in range(num_epochs):\n",
    "    for label, sent in data:\n",
    "        ids = []\n",
    "        for word in sent:\n",
    "            index = word2idx.get(word)\n",
    "            if index:\n",
    "                ids.append([word2idx[word]])\n",
    "            else:\n",
    "                ids.append([unk])\n",
    "        batch_labels = torch.tensor([label])\n",
    "        sentence = torch.tensor(ids).squeeze()\n",
    "        optimizer.zero_grad()\n",
    "        batch_outputs = ModelLSTMM(sentence)\n",
    "        loss = criterion(batch_outputs, batch_labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "# print(\"loss\", loss.item())\n",
    "# print(\"epoch\", epoch)\n",
    "print(\"training finished\")\n",
    "print(\"accuracy=\",lstm_accc(data_test)*100)\n",
    "# return ModelLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_accc(data_test):\n",
    "    test_len =len(data_test)\n",
    "    unk = word2idx.get('#UNK#')\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        for label,sent in data_test:\n",
    "            ids=[]\n",
    "            #unk = 3375\n",
    "            labels=[]\n",
    "            for word in sent:\n",
    "                index =word2idx.get(word)\n",
    "                if index:\n",
    "                    ids.append([word2idx[word]])\n",
    "                else:\n",
    "                    ids.append([unk])\n",
    "            sentence=torch.tensor(ids).squeeze()\n",
    "            batch_outputs = ModelLSTMM(sentence)\n",
    "            predicted = np.argmax(F.softmax(batch_outputs).data.numpy())\n",
    "            if predicted==label:\n",
    "                 correct+=1\n",
    "#             print('prediction: {}'.format(predicted))\n",
    "#             print('actual: {}'.format(label))\n",
    "        accuracy = correct/test_len\n",
    "#         print('accuracy: {}'.format(accuracy))\n",
    "        return accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
