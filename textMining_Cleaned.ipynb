{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from collections import Counter\n",
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    f = open(path, \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    data = []\n",
    "    for l in lines:\n",
    "        labelSplit = l.replace('\\n','').split(' ', 1)\n",
    "        data.append([labelSplit[0], [word.lower() for word in labelSplit[1].split()]])\n",
    "    return data\n",
    "\n",
    "data = read_data('./questions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data, path):\n",
    "    stop_words = []\n",
    "    with open(path) as f:\n",
    "        stop_words = [word for line in f for word in line.split(\",\")]\n",
    "    data_without_stop_words = []\n",
    "    for k, v in data:\n",
    "        words = [t for t in v if t not in stop_words]\n",
    "        data_without_stop_words.append((k, words))\n",
    "    return data_without_stop_words\n",
    "\n",
    "data = remove_stop_words(data, './stop_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(data):\n",
    "    _labels = []\n",
    "    for k,v in data:\n",
    "        _labels.append(k)   \n",
    "    _unique_label = list(set(_labels))\n",
    "    _unique_label_dict = {}\n",
    "    for k,v in enumerate(_unique_label):\n",
    "        _unique_label_dict[v] = k\n",
    "    return _unique_label_dict\n",
    "\n",
    "labels = get_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_labels(data, labels):        \n",
    "    cleaned_data = []\n",
    "    for k,v in data:\n",
    "        cleaned_data.append((labels[k],v))\n",
    "        \n",
    "    return np.array(cleaned_data)\n",
    "\n",
    "data = append_labels(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indexed_vocab(data):\n",
    "    vocab = []\n",
    "    for _, sent in data:\n",
    "        for word in sent:\n",
    "            vocab.append(word)\n",
    "    count = Counter(vocab)\n",
    "    count = {w : count[w] for w in count if count[w] >= 2}\n",
    "    vocab = []\n",
    "    for k, v in count.items():\n",
    "        vocab.append(k)\n",
    "    indexed_vocab = {word: idx for idx, word in enumerate(vocab)}\n",
    "    return indexed_vocab\n",
    "def create_vocab(data):\n",
    "    total_words_orig = []\n",
    "    for k,sent in data:\n",
    "        for word in sent:\n",
    "            total_words_orig.append(word)\n",
    "    total_words = list(set(total_words_orig))\n",
    "    total_words_str = ' '.join(total_words)\n",
    "    vocab = set(total_words_str.split()) \n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)} # create word index\n",
    "    return word2idx\n",
    "word2idx=create_vocab(data)\n",
    "word2idx['#UNK#'] = len(word2idx)\n",
    "indexed_vocab = create_indexed_vocab(data)\n",
    "indexed_vocab['#UNK#'] = len(indexed_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path, indexed_vocab, embedding_dim=300):\n",
    "    with open(path) as f:\n",
    "        embeddings = np.zeros((len(indexed_vocab), embedding_dim))\n",
    "        for line in f.readlines():\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            index = indexed_vocab.get(word)\n",
    "            if index:\n",
    "                vector = np.array(values[1:], dtype='float32')\n",
    "                embeddings[index] = vector\n",
    "        return torch.from_numpy(embeddings).float()\n",
    "\n",
    "glove_random = load_glove_embeddings('./glove.small.txt', indexed_vocab)\n",
    "glove_pre = load_glove_embeddings('./glove.small.txt', word2idx)\n",
    "embeddings_random = nn.Embedding(glove_random.size(0), glove_random.size(1))\n",
    "embeddings_pretrained = nn.Embedding.from_pretrained(glove_pre, freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, test_ratio):\n",
    "    data_copy = copy.deepcopy(data)\n",
    "    np.random.shuffle(data_copy)\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test = data_copy[:test_set_size]\n",
    "    train = data_copy[test_set_size:]\n",
    "    return train, test\n",
    "\n",
    "train, test = split_train_test(data, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWClassifier(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size, num_labels):\n",
    "        super(BOWClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(self.input_size, num_labels)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(x)\n",
    "        #output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4362, 1, 300])\n",
      "tensor([[[-0.0886,  0.1401, -0.1218,  ..., -0.0261,  0.0924,  0.1195]],\n",
      "\n",
      "        [[-0.0574,  0.2847, -0.1079,  ...,  0.0217,  0.0816,  0.2187]],\n",
      "\n",
      "        [[-0.0646,  0.1491,  0.1250,  ..., -0.0814,  0.0329, -0.0212]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.0319,  0.3197,  0.0022,  ..., -0.0563,  0.1042, -0.0492]],\n",
      "\n",
      "        [[-0.0812,  0.1550, -0.2108,  ...,  0.0016,  0.0891,  0.0668]],\n",
      "\n",
      "        [[ 0.0192,  0.1936,  0.0139,  ...,  0.0501,  0.2304,  0.3875]]])\n",
      "tensor([[32],\n",
      "        [37],\n",
      "        [42],\n",
      "        [35],\n",
      "        [17],\n",
      "        [48],\n",
      "        [13],\n",
      "        [48],\n",
      "        [ 1],\n",
      "        [43]])\n"
     ]
    }
   ],
   "source": [
    "def make_bow_vector(sentence, indexed_vocab):\n",
    "    pt_tensor= torch.zeros(300, dtype=torch.long)\n",
    "    count = 0\n",
    "    for word in sentence:\n",
    "        count += 1\n",
    "        if word in indexed_vocab:\n",
    "            pt_tensor = torch.add(pt_tensor, embeddings_pretrained(torch.LongTensor([indexed_vocab[word]]))[0])\n",
    "        else:\n",
    "            pt_tensor = torch.add(pt_tensor, embeddings_pretrained(torch.LongTensor([indexed_vocab['#UNK']]))[0])\n",
    "    pt_tensor=torch.div(pt_tensor, count)\n",
    "    return pt_tensor\n",
    "\n",
    "def get_bow_rep(data):\n",
    "    bow_data = []\n",
    "    for label, sent in data:\n",
    "        bow_data.append(make_bow_vector(sent, word2idx).reshape(-1, 300))\n",
    "    return torch.stack(bow_data)\n",
    "        \n",
    "training_set = get_bow_rep(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = 300\n",
    "hidden_size = 100\n",
    "num_classes = 51\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model = BOWClassifier(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 3.2282\n",
      "Epoch [1/50], Loss: 4.4131\n",
      "Epoch [1/50], Loss: 3.3566\n",
      "Epoch [1/50], Loss: 3.4310\n",
      "Epoch [1/50], Loss: 2.9971\n",
      "Epoch [1/50], Loss: 3.0752\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-3ee8525309df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mi\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mbow_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmake_bow_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword2idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mbow_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbow_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-48-a23d1c173980>\u001b[0m in \u001b[0;36mmake_bow_vector\u001b[1;34m(sentence, indexed_vocab)\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mcount\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mindexed_vocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m             \u001b[0mpt_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpt_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexed_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mpt_tensor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpt_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membeddings_pretrained\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindexed_vocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'#UNK'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\sparse.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    112\u001b[0m         return F.embedding(\n\u001b[0;32m    113\u001b[0m             \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36membedding\u001b[1;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[0;32m   1482\u001b[0m         \u001b[1;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1483\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1484\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    permutation = torch.randperm(len(train))\n",
    "    i = 0\n",
    "    for label, question in train[permutation]:\n",
    "        i += 1\n",
    "        optimizer.zero_grad()\n",
    "        bow_vec = make_bow_vector(question, word2idx)\n",
    "        bow_vec = bow_vec.reshape(-1, 300).to(device)\n",
    "        label = torch.LongTensor([label])\n",
    "        label = label.to(device)\n",
    "        output = model(bow_vec)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    permutation = torch.randperm(training_set.size()[0])\n",
    "    for i in range(0, training_set.size()[0], batch_size):\n",
    "        opt.zero_grad()\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        batch_features = training_set[indices]\n",
    "        batch_features = batch_features.reshape(-1, 300).to(device)\n",
    "        batch_labels = torch.LongTensor([label for label, sent in train[indices]])\n",
    "        batch_features = batch_features.reshape(-1, 1).to(device)\n",
    "        batch_outputs = model(batch_features)\n",
    "        loss = loss_function(batch_outputs, batch_labels)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[ 0.1588,  0.1454,  0.1403,  ...,  0.1427,  0.0615,  0.0544]],\n",
      "\n",
      "        [[ 0.1684,  0.1359,  0.0865,  ..., -0.0125,  0.0972, -0.1372]],\n",
      "\n",
      "        [[-0.0158,  0.1209,  0.1114,  ..., -0.1614,  0.0982,  0.1127]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[ 0.1571,  0.0427,  0.3998,  ...,  0.0472,  0.1954, -0.0246]],\n",
      "\n",
      "        [[ 0.0224,  0.0198, -0.2315,  ...,  0.1584,  0.1841,  0.0574]],\n",
      "\n",
      "        [[-0.1022,  0.2185, -0.0167,  ...,  0.0636,  0.1347,  0.0565]]])\n",
      "tensor([[17],\n",
      "        [17],\n",
      "        [42],\n",
      "        [ 1],\n",
      "        [45],\n",
      "        [39],\n",
      "        [45],\n",
      "        [13],\n",
      "        [17],\n",
      "        [12],\n",
      "        [ 1],\n",
      "        [32],\n",
      "        [34],\n",
      "        [17],\n",
      "        [37],\n",
      "        [48],\n",
      "        [46],\n",
      "        [39],\n",
      "        [17],\n",
      "        [45],\n",
      "        [11],\n",
      "        [17],\n",
      "        [46],\n",
      "        [ 1],\n",
      "        [37],\n",
      "        [ 3],\n",
      "        [17],\n",
      "        [10],\n",
      "        [37],\n",
      "        [ 1],\n",
      "        [49],\n",
      "        [42],\n",
      "        [13],\n",
      "        [45],\n",
      "        [50],\n",
      "        [ 4],\n",
      "        [39],\n",
      "        [42],\n",
      "        [46],\n",
      "        [45],\n",
      "        [32],\n",
      "        [17],\n",
      "        [11],\n",
      "        [39],\n",
      "        [17],\n",
      "        [32],\n",
      "        [12],\n",
      "        [12],\n",
      "        [11],\n",
      "        [45],\n",
      "        [17],\n",
      "        [13],\n",
      "        [ 1],\n",
      "        [ 1],\n",
      "        [10],\n",
      "        [50],\n",
      "        [17],\n",
      "        [17],\n",
      "        [39],\n",
      "        [45],\n",
      "        [17],\n",
      "        [32],\n",
      "        [47],\n",
      "        [24],\n",
      "        [15],\n",
      "        [17],\n",
      "        [ 1],\n",
      "        [10],\n",
      "        [30],\n",
      "        [39],\n",
      "        [39],\n",
      "        [17],\n",
      "        [20],\n",
      "        [13],\n",
      "        [17],\n",
      "        [37],\n",
      "        [32],\n",
      "        [32],\n",
      "        [17],\n",
      "        [37],\n",
      "        [ 1],\n",
      "        [13],\n",
      "        [32],\n",
      "        [ 1],\n",
      "        [22],\n",
      "        [12],\n",
      "        [ 1],\n",
      "        [17],\n",
      "        [42],\n",
      "        [48],\n",
      "        [42],\n",
      "        [11],\n",
      "        [11],\n",
      "        [39],\n",
      "        [17],\n",
      "        [49],\n",
      "        [33],\n",
      "        [ 6],\n",
      "        [32],\n",
      "        [38]])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected target size (100, 51), got torch.Size([100, 1])",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-54-db42bba73f2a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mbatch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_features\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    914\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    915\u001b[0m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[1;32m--> 916\u001b[1;33m                                ignore_index=self.ignore_index, reduction=self.reduction)\n\u001b[0m\u001b[0;32m    917\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   2019\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2020\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2021\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2022\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2023\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1846\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1847\u001b[0m             raise ValueError('Expected target size {}, got {}'.format(\n\u001b[1;32m-> 1848\u001b[1;33m                 out_size, target.size()))\n\u001b[0m\u001b[0;32m   1849\u001b[0m         \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1850\u001b[0m         \u001b[0mtarget\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected target size (100, 51), got torch.Size([100, 1])"
     ]
    }
   ],
   "source": [
    "#training in batches\n",
    "batch_size=100\n",
    "for epoch in range(num_epochs):\n",
    "    permutation = torch.randperm(training_set.size()[0])\n",
    "    for i in range(0, training_set.size()[0], batch_size):\n",
    "        optimizer.zero_grad()\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        batch_features = training_set[indices]\n",
    "        batch_features=batch_features.to(device)\n",
    "        print(batch_features)\n",
    "        bow_label = []\n",
    "        for label, sent in train[indices]:\n",
    "            label = torch.LongTensor([label])\n",
    "            bow_label.append(label)\n",
    "        batch_labels=torch.stack(bow_label).to(device)\n",
    "        print(batch_labels)\n",
    "        batch_outputs = model(batch_features)\n",
    "        loss = criterion(batch_outputs, batch_labels)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "test_len =len(test)\n",
    "correct=0\n",
    "for label,data in test:\n",
    "    bow_vec = make_bow_vector(data, word2idx)\n",
    "    logprobs = bow(bow_vec)\n",
    "    print(logprobs)\n",
    "    pred = np.argmax(logprobs.data.numpy())\n",
    "    if pred==label:\n",
    "        correct+=1\n",
    "    print('prediction: {}'.format(pred))\n",
    "    print('actual: {}'.format(label))\n",
    "accuracy = correct/test_len\n",
    "print('accuracy: {}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
