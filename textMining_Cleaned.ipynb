{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from collections import Counter\n",
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    f = open(path, \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    data = []\n",
    "    for l in lines:\n",
    "        labelSplit = l.replace('\\n','').split(' ', 1)\n",
    "        data.append([labelSplit[0], [word.lower() for word in labelSplit[1].split()]])\n",
    "    return data \n",
    "\n",
    "data = read_data('./questions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data, path):\n",
    "    stop_words = []\n",
    "    with open('stop_words.txt') as f:\n",
    "        stop_words = [word for line in f for word in line.split(\",\")]\n",
    "    data_without_stop_words = []\n",
    "    for k, v in data:\n",
    "        words = [t for t in v if t not in stop_words]\n",
    "        data_without_stop_words.append((k, words))\n",
    "    return data_without_stop_words\n",
    "\n",
    "data = remove_stop_words(data, './stop_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(data):\n",
    "    _labels = []\n",
    "    for k,v in data:\n",
    "        _labels.append(k)   \n",
    "    _unique_label = list(set(_labels))\n",
    "    _unique_label_dict = {}\n",
    "    for k,v in enumerate(_unique_label):\n",
    "        _unique_label_dict[v] = k\n",
    "    return _unique_label_dict\n",
    "\n",
    "labels = get_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(data, labels):\n",
    "    encoded_labels = {}\n",
    "    for i in range(0, 50): \n",
    "        label = [0] * 50\n",
    "        label[i] = 1\n",
    "        encoded_labels[i] = label\n",
    "        \n",
    "    cleaned_data = []\n",
    "    for k,v in data:\n",
    "        cleaned_data.append((encoded_labels[labels[k]],v))\n",
    "        \n",
    "    return cleaned_data\n",
    "\n",
    "data = encode_labels(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indexed_vocab(data):\n",
    "    vocab = []\n",
    "    for _, sent in data:\n",
    "        for word in sent:\n",
    "            vocab.append(word)\n",
    "    count = Counter(vocab)\n",
    "    count = {w : count[w] for w in count if count[w] >= 2}\n",
    "    vocab = []\n",
    "    for k, v in count.items():\n",
    "        vocab.append(k)\n",
    "    indexed_vocab = {word: idx for idx, word in enumerate(vocab)}\n",
    "    return indexed_vocab\n",
    "\n",
    "indexed_vocab = create_indexed_vocab(data)\n",
    "indexed_vocab['#UNK#'] = len(indexed_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path, indexed_vocab, embedding_dim=300):\n",
    "    with open(path) as f:\n",
    "        embeddings = np.zeros((len(indexed_vocab), embedding_dim))\n",
    "        for line in f.readlines():\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            index = indexed_vocab.get(word)\n",
    "            if index:\n",
    "                vector = np.array(values[1:], dtype='float32')\n",
    "                embeddings[index] = vector\n",
    "            else:\n",
    "                vector = np.array(values[1:], dtype='float32')\n",
    "                embeddings[len(indexed_vocab) - 1] = vector\n",
    "        return torch.from_numpy(embeddings).float()\n",
    "\n",
    "glove = load_glove_embeddings('./glove.txt', indexed_vocab)\n",
    "embeddings_random = nn.Embedding(glove.size(0), glove.size(1))\n",
    "embeddings_pretrained = nn.Embedding.from_pretrained(glove, freeze=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, test_ratio):\n",
    "    data_copy = copy.deepcopy(data)\n",
    "    np.random.shuffle(data_copy)\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test = data_copy[:test_set_size]\n",
    "    train = data_copy[test_set_size:]\n",
    "    return train, test\n",
    "\n",
    "train, test = split_train_test(data, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWClassifier(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size, num_labels):\n",
    "        super(BOWClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(self.hidden_size,num_labels)\n",
    "        #self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(relu)\n",
    "        #output = self.sigmoid(output)\n",
    "        return F.softmax(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 300\n",
    "hidden_size = 10\n",
    "num_labels = 50\n",
    "bow = BOWClassifier(input_size, hidden_size, num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, indexed_vocab):\n",
    "    vec = [0] * 300\n",
    "    pt_tensor= torch.FloatTensor(vec)\n",
    "    count = 0\n",
    "    for word in sentence:\n",
    "        count+=1\n",
    "        if word not in indexed_vocab:\n",
    "            pt_tensor = torch.add(pt_tensor, embeddings_pretrained(torch.LongTensor([indexed_vocab['#UNK#']]))[0])\n",
    "        else:\n",
    "            pt_tensor = torch.add(pt_tensor, embeddings_pretrained(torch.LongTensor([indexed_vocab[word]]))[0])\n",
    "    pt_tensor=torch.div(pt_tensor,count)\n",
    "    return pt_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#loss_function = nn.MSELoss()\n",
    "loss_function = nn.NLLLoss()\n",
    "opt = torch.optim.SGD(bow.parameters(), lr = 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Expected 2 or more dimensions (got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-3e83b85cd5b4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mll\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mll\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;31m#print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m    530\u001b[0m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    531\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 532\u001b[1;33m             \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    533\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    534\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m    202\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction)\u001b[0m\n\u001b[0;32m   1830\u001b[0m     \u001b[0mdim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1831\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdim\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1832\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Expected 2 or more dimensions (got {})'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1833\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1834\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Expected 2 or more dimensions (got 1)"
     ]
    }
   ],
   "source": [
    "\n",
    "epoch = 2\n",
    "for epoch in range(epoch):\n",
    "    for l, d in data:\n",
    "        opt.zero_grad()\n",
    "        bow_vec = make_bow_vector(d, indexed_vocab) \n",
    "        y_pred = bow(bow_vec)\n",
    "        ll=[]\n",
    "        ll.append(labels[l])\n",
    "        loss = loss_function(y_pred,ll)\n",
    "        #print('Epoch {}: train loss: {}'.format(epoch, loss.item()))\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ï»¿DESC:manner', ['serfdom', 'develop', 'leave', 'russia', '?']), ('ENTY:cremat', ['films', 'featured', 'character', 'popeye', 'doyle', '?']), ('DESC:manner', ['find', 'list', 'celebrities', \"'\", 'real', 'names', '?']), ('ENTY:animal', ['fowl', 'grabs', 'spotlight', 'chinese', 'year', 'monkey', '?'])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\joshy\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "#sample code\n",
    "b=data[0:4]\n",
    "print(b)\n",
    "cdata=[]\n",
    "target=[]\n",
    "for l,d in b:\n",
    "    bow_vec = make_bow_vector(d, indexed_vocab)\n",
    "    cdata.append(list(bow_vec.tolist()))\n",
    "    target.append(labels[l])\n",
    "c_tensor= torch.FloatTensor(cdata)\n",
    "t_tensor=torch.LongTensor(target)\n",
    "y_pred = bow(c_tensor)\n",
    "loss = loss_function(y_pred,t_tensor)\n",
    "loss.backward()\n",
    "opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# batch_size =4 # or whatever\n",
    "# epochs = 2\n",
    "# for epoch in range(epochs):\n",
    "#     for i in range(0,len(data), batch_size):\n",
    "#         opt.zero_grad()\n",
    "#         b=data[i:batch_size]\n",
    "#         cdata=[]\n",
    "#         target=[]\n",
    "#         for l,d in b:\n",
    "#             bow_vec = make_bow_vector(d, indexed_vocab)\n",
    "#             cdata.append(list(bow_vec.tolist()))\n",
    "#             target.append(labels[l])\n",
    "#         c_tensor= torch.FloatTensor(cdata)\n",
    "#         t_tensor=torch.LongTensor(target)\n",
    "#         y_pred = bow(c_tensor)\n",
    "#         loss = loss_function(y_pred,t_tensor)\n",
    "#         loss.backward()\n",
    "#         opt.step()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
