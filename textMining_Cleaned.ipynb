{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import copy\n",
    "from collections import Counter\n",
    "import torch \n",
    "import numpy as np\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(path):\n",
    "    f = open(path, \"r\")\n",
    "    lines = f.readlines()\n",
    "    f.close()\n",
    "    data = []\n",
    "    for l in lines:\n",
    "        labelSplit = l.replace('\\n','').split(' ', 1)\n",
    "        data.append([labelSplit[0], [word.lower() for word in labelSplit[1].split()]])\n",
    "    return data\n",
    "\n",
    "data = read_data('./questions.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stop_words(data, path):\n",
    "    stop_words = []\n",
    "    with open(path) as f:\n",
    "        stop_words = [word for line in f for word in line.split(\",\")]\n",
    "    data_without_stop_words = []\n",
    "    for k, v in data:\n",
    "        words = [t for t in v if t not in stop_words]\n",
    "        data_without_stop_words.append((k, words))\n",
    "    return data_without_stop_words\n",
    "\n",
    "data = remove_stop_words(data, './stop_words.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(data):\n",
    "    _labels = []\n",
    "    for k,v in data:\n",
    "        _labels.append(k)   \n",
    "    _unique_label = list(set(_labels))\n",
    "    _unique_label_dict = {}\n",
    "    for k,v in enumerate(_unique_label):\n",
    "        _unique_label_dict[v] = k\n",
    "    return _unique_label_dict\n",
    "\n",
    "labels = get_labels(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_labels(data, labels):        \n",
    "    cleaned_data = []\n",
    "    for k,v in data:\n",
    "        cleaned_data.append((labels[k],v))\n",
    "        \n",
    "    return np.array(cleaned_data)\n",
    "\n",
    "data = append_labels(data, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_indexed_vocab(data):\n",
    "    vocab = []\n",
    "    for _, sent in data:\n",
    "        for word in sent:\n",
    "            vocab.append(word)\n",
    "    count = Counter(vocab)\n",
    "    count = {w : count[w] for w in count if count[w] >= 2}\n",
    "    vocab = []\n",
    "    for k, v in count.items():\n",
    "        vocab.append(k)\n",
    "    indexed_vocab = {word: idx for idx, word in enumerate(vocab)}\n",
    "    return indexed_vocab\n",
    "def create_vocab(data):\n",
    "    total_words_orig = []\n",
    "    for k,sent in data:\n",
    "        for word in sent:\n",
    "            total_words_orig.append(word)\n",
    "    total_words = list(set(total_words_orig))\n",
    "    total_words_str = ' '.join(total_words)\n",
    "    vocab = set(total_words_str.split()) \n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)} # create word index\n",
    "    return word2idx\n",
    "word2idx=create_vocab(data)\n",
    "word2idx['#UNK#'] = len(word2idx)\n",
    "indexed_vocab = create_indexed_vocab(data)\n",
    "indexed_vocab['#UNK#'] = len(indexed_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_glove_embeddings(path, indexed_vocab, embedding_dim=300):\n",
    "    with open(path) as f:\n",
    "        embeddings = np.zeros((len(indexed_vocab), embedding_dim))\n",
    "        for line in f.readlines():\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            index = indexed_vocab.get(word)\n",
    "            if index:\n",
    "                vector = np.array(values[1:], dtype='float32')\n",
    "                embeddings[index] = vector\n",
    "        return torch.from_numpy(embeddings).float()\n",
    "\n",
    "glove_random = load_glove_embeddings('./glove.small.txt', indexed_vocab)\n",
    "glove_pre = load_glove_embeddings('./glove.small.txt', word2idx)\n",
    "embeddings_random = nn.Embedding(glove_random.size(0), glove_random.size(1))\n",
    "embeddings_pretrained = nn.Embedding.from_pretrained(glove_pre, freeze=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_test(data, test_ratio):\n",
    "    data_copy = copy.deepcopy(data)\n",
    "    np.random.shuffle(data_copy)\n",
    "    test_set_size = int(len(data) * test_ratio)\n",
    "    test = data_copy[:test_set_size]\n",
    "    train = data_copy[test_set_size:]\n",
    "    return train, test\n",
    "\n",
    "train, test = split_train_test(data, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOWClassifier(nn.Module):\n",
    "    def __init__(self,input_size,hidden_size, num_labels):\n",
    "        super(BOWClassifier, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size  = hidden_size\n",
    "        self.fc1 = torch.nn.Linear(self.input_size, self.hidden_size)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "        self.fc2 = torch.nn.Linear(self.input_size, num_labels)\n",
    "        self.sigmoid = torch.nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden = self.fc1(x)\n",
    "        relu = self.relu(hidden)\n",
    "        output = self.fc2(x)\n",
    "        #output = self.sigmoid(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_bow_vector(sentence, indexed_vocab):\n",
    "    pt_tensor= torch.zeros(300, dtype=torch.long)\n",
    "    count = 0\n",
    "    for word in sentence:\n",
    "        count += 1\n",
    "        if word in indexed_vocab:\n",
    "            pt_tensor = torch.add(pt_tensor, embeddings_pretrained(torch.LongTensor([indexed_vocab[word]]))[0])\n",
    "        else:\n",
    "            pt_tensor = torch.add(pt_tensor, embeddings_pretrained(torch.LongTensor([indexed_vocab['#UNK']]))[0])\n",
    "    pt_tensor=torch.div(pt_tensor, count)\n",
    "    return pt_tensor\n",
    "\n",
    "def get_bow_rep(data):\n",
    "    bow_data = []\n",
    "    for label, sent in data:\n",
    "        bow_data.append(make_bow_vector(sent, word2idx))\n",
    "    return torch.stack(bow_data)\n",
    "        \n",
    "training_set = get_bow_rep(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = 300\n",
    "hidden_size = 100\n",
    "num_classes = 51\n",
    "num_epochs = 50\n",
    "learning_rate = 0.0001\n",
    "\n",
    "model = BOWClassifier(input_size, hidden_size, num_classes).to(device)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 4.0518\n",
      "Epoch [1/50], Loss: 3.7560\n",
      "Epoch [1/50], Loss: 3.7225\n",
      "Epoch [1/50], Loss: 3.6000\n",
      "Epoch [1/50], Loss: 3.8785\n",
      "Epoch [1/50], Loss: 3.5043\n",
      "Epoch [1/50], Loss: 3.7926\n",
      "Epoch [1/50], Loss: 3.4167\n",
      "Epoch [1/50], Loss: 3.6006\n",
      "Epoch [1/50], Loss: 3.2649\n",
      "Epoch [1/50], Loss: 3.3179\n",
      "Epoch [1/50], Loss: 3.2121\n",
      "Epoch [1/50], Loss: 3.1552\n",
      "Epoch [1/50], Loss: 3.2205\n",
      "Epoch [1/50], Loss: 3.3187\n",
      "Epoch [1/50], Loss: 4.6764\n",
      "Epoch [1/50], Loss: 3.6325\n",
      "Epoch [1/50], Loss: 3.1006\n",
      "Epoch [1/50], Loss: 2.6358\n",
      "Epoch [1/50], Loss: 3.6131\n",
      "Epoch [1/50], Loss: 2.5636\n",
      "Epoch [1/50], Loss: 3.0848\n",
      "Epoch [1/50], Loss: 3.4106\n",
      "Epoch [1/50], Loss: 3.5063\n",
      "Epoch [1/50], Loss: 3.8523\n",
      "Epoch [1/50], Loss: 3.2214\n",
      "Epoch [1/50], Loss: 2.6263\n",
      "Epoch [1/50], Loss: 3.0389\n",
      "Epoch [1/50], Loss: 3.9064\n",
      "Epoch [1/50], Loss: 3.1807\n",
      "Epoch [1/50], Loss: 3.4979\n",
      "Epoch [1/50], Loss: 2.6460\n",
      "Epoch [1/50], Loss: 2.7840\n",
      "Epoch [1/50], Loss: 4.3480\n",
      "Epoch [1/50], Loss: 2.4595\n",
      "Epoch [1/50], Loss: 3.2974\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-147-3ee8525309df>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mbow_vec\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbow_vec\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLongTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m         \u001b[0mlabel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbow_vec\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    permutation = torch.randperm(len(train))\n",
    "    i = 0\n",
    "    for label, question in train[permutation]:\n",
    "        i += 1\n",
    "        optimizer.zero_grad()\n",
    "        bow_vec = make_bow_vector(question, word2idx)\n",
    "        bow_vec = bow_vec.reshape(-1, 300).to(device)\n",
    "        label = torch.LongTensor([label])\n",
    "        label = label.to(device)\n",
    "        output = model(bow_vec)\n",
    "        loss = criterion(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    permutation = torch.randperm(training_set.size()[0])\n",
    "    for i in range(0, training_set.size()[0], batch_size):\n",
    "        opt.zero_grad()\n",
    "        indices = permutation[i:i + batch_size]\n",
    "        batch_features = training_set[indices]\n",
    "        batch_features = batch_features.reshape(-1, 300).to(device)\n",
    "        batch_labels = torch.LongTensor([label for label, sent in train[indices]])\n",
    "        batch_features = batch_features.reshape(-1, 1).to(device)\n",
    "        batch_outputs = model(batch_features)\n",
    "        loss = loss_function(batch_outputs, batch_labels)\n",
    "        print(loss)\n",
    "        loss.backward()\n",
    "        opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cdata=[]\n",
    "# target = \n",
    "# for l,d in b:\n",
    "#     bow_vec = make_bow_vector(d, indexed_vocab)\n",
    "#     cdata.append(list(bow_vec.tolist()))\n",
    "#     target.append(labels[l])\n",
    "# c_tensor= torch.FloatTensor(cdata)\n",
    "# t_tensor=torch.LongTensor(target)\n",
    "# y_pred = bow(c_tensor)\n",
    "# loss = loss_function(y_pred,t_tensor)\n",
    "# loss.backward()\n",
    "# opt.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing\n",
    "test_len =len(test)\n",
    "correct=0\n",
    "for label,data in test:\n",
    "    bow_vec = make_bow_vector(data, word2idx)\n",
    "    logprobs = bow(bow_vec)\n",
    "    print(logprobs)\n",
    "    pred = np.argmax(logprobs.data.numpy())\n",
    "    if pred==label:\n",
    "        correct+=1\n",
    "    print('prediction: {}'.format(pred))\n",
    "    print('actual: {}'.format(label))\n",
    "accuracy = correct/test_len\n",
    "print('accuracy: {}'.format(accuracy))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
